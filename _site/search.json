[
  
    {
      "title"       : "Kafka Connect: A love/hate relationship",
      "category"    : "",
      "tags"        : "kafka, streaming, event sourcing, connect, databases",
      "url"         : "./kconnect-issues-and-magic.html",
      "date"        : "2023-10-10 18:00:00 +0000",
      "description" : "A write up on what makes Kafka Connect such a pain to work with - yet so useful in the building of real time data pipelines. We'll discuss the Postgres connector as the main example, and provide some gotchas that are general to the tool.",
      "content"     : "Apache Kafka is the de-facto streaming platform for businesses today, and with it gaining popularity, so has the associated subproject “Kafka Connect”. Kafka Connect allows to use pre-made “Connectors” to send and receive data to/from Kafka. They require no code, it is all configuration driven. This provides a lot of advantages and a lot of headaches. In this blog we’ll talk about both, how to get around them, and hopefully there will be enough information to help you make good decisions around its usage.We will use the community’s Debezium Postgres Connector to talk about sourcing data from Postgres to Kafka, and Confluent’s JDBC Sink Connector to talk about sending data from Kafka to another database.A primer on how Kafka Connect worksThe advent of Cloud has abstracted a lot on how things run, and Confluent Cloud does a good job at hiding the complexity. However, I believe understanding how something works really allows you to make the best decisions around whether it is a good fit for you. So here is a quick primer: Kafka Connect is a distributed system, separate from Kafka, but powered by it Kafka Connect is a platform, Connectors run on it Kafka Connect has source connectors which bring data from systems into Kafka, and sink connectors which send data from Kafka into end systems Kafka Connect uses an internal topic in Kafka to coordinate source connectors, and the consumer group protocol to coordinate sink connectors Connectors aren’t made by one organization, they are made by the community, and the community is strong - providing 100s of connectors for organizations to use. However, this also means that quality and feature sets vary greatlyThat should give us a good enough base to start talking about how things work. Lets get started.Source ConnectorsOn sourcing from databasesPostgres is a very popular database out there, which makes the Postgres connector a popular connector as well. Thankfully, the community has rallied around Debezium as the project to congregate and work on very important connectors, and Postgres is one of them.Sourcing from databases is hard, if you take some time to look into how Debezium has standardized in reading from WALs, you’ll realize is no joke to ensure no data loss, no data duplication, as well as high uptime. Once you’ve made this realization we are set: We need the community’s standard connector to send data from Postgres to Kafka instead of doing our own thing and most likely not do as good of a job (I’ve seen plenty of companies that do this, for some reason). Now that we’ve made the commitment to use the connector, we have to understand it. How does it all work?To read data from a database, it would be extremely inefficient and wasteful to issue query reads and then send that data back to Kafka (although there is a connector that does that if you wish) so instead, what most connectors will do is listen to the Database’s Write Ahead Log (WAL). Some DB’s provide direct and easy access to their WAL, which is nice! would be so jerkish not to! Anyways, Postgres does, and it does so through “logical decoding” (their fancy name for it). Here lies our first blessing / problem.Reading from WALsThere are 3 common problems that plague users of Kafka Connect whenever using a Change Data Capture (CDC) connector such as the Debezium Postgres connector: Old version of Postgres: Postgres has been around for a while, but logical decoding hasn’t, and formal first-class support is even more recent. Users of old versions of Postgres will have to utilize a community decoder plugin. These have been found to have many issues, and some of them have workarounds (too many for us to get into this post), but this will surely slow down deployment speed and resilience. Worth nothing that newer versions have a standard decoder called pgoutput that performs very well and is backed by the project as a whole. Size of the WAL: Obviously databases have to be space conscious, and they don’t keep the WAL forever; However, in streaming we’ve directly plugged into it and one day the connector might fail but the database will keep chugging along… which means that when the connector comes back, it has to catch up. What if data that it needs to catch up to has already been deleted by Postgres? Using the Postgres also means negotiating with your DB Admin how long the WAL can be kept, and that becomes the maximum amount of time you have to resolve an issue with the connector… before you have to start recovery actions. Most source connectors don’t offer exactly once (yet): This is true for Debezium connectors, which means that when it all goes wrong, you will have duplicates (but no data loss, as they do offere at least once). For MOST applications, this is sufficient, but a lot of people go into this connector expecting exactly once deliveries.The above issues make the primordial task of the connector (getting data out of the system) quite the math equation that has to be solved. What is important is that it CAN be solved, but does require careful tuning and testing. This pain get exacerbated by the very common situation that the people developing data streaming flows aren’t DB Admins, so they would like not to worry about these.Anyways, you’ve tamed the sourcing of the data, then you need to ensure it is actually sent in a way that is useful, which is where pain #2 pops up.Delivery and interpretation of database records in KafkaData Types. They make a developers life harder and easier at the same time. Can’t live without them.When sourcing data from Databases in order for it to be usable somewhere else, you can imagine maintaining data types or at least converting them properly is paramount. With such requirements, the Kafka community has been evolving in their idea on how to handle these, here are some challenges that come with the territory: Data interpretation and evolution is hard: Everything that goes into Kafka is serialized as a byte array, which means that when read it requires a way to deserialize it. By default, Kafka provides a JSON serializer, but as you probably know, sending large amounts of data with naive JSON serialization is actually really inefficient. Confluent has had the Schema Registry for a while to address this issue as well as Data Governance and Data Quality problems. However, there are various solutions in the community that aim to help solve these problems. (I go quite into depth on this issue in a podcast with the great Kris Jenkins). This is actually a multi-tiered issue: When consumed from the Database, data comes with the DBs data type When converted into a Kafka Connect record, it gets casted into a set of types pre-defined and available in documentation When sent into Kafka, the data is type-less When consumed, you have to cast again, either to connect’s internal data types (which shouldn’t be a problem since it was sourced with connect) or to a data type that is useful for the application. ksqlDB famously does not have good compatibility with all the possible data types a DB may have.Long story short, be careful around what your data gets converted to, and ensure you are comfortable with the casting. There are various connector configurations to change the behavior here, specially around time (always the easiest problem to solve in production systems :) /s) I highly recommend utilizing Schema Registry here. In-flight transforms: One of the more useful features in Kafka Connect is the ability to modify every record that passes through it slightly to fit your needs. These transformations are pluggable and the community has a sprawl of them. Debezium has some that are very helpful. One of the problems that get developer teams is that because of how useful it would be to just modify data on the fly before even storing it, they try to fit all data transformations in these and you can get pretty creative given correctly chaining transforms and predicates. However, the reality is that these are meant to be lightweight because they can severely hamper performance of the connector. If you really need involved transformations I would try Kafka Streams, Apache Flink, or ksqlDB. Scalability: More often than not, the connector will be faster than whatever system you are sourcing from, which is mostly great! However, something to note is that most source connectors will be limited to 1 task (tasks are the unit of parallelism in Kafka Connect) due to ensuring total order of updates from the database. This may hamper you, specifically if you have expensive transforms in it. A good rule of thumb is to separate table updates into various connectors in order to ensure being able to keep up for high traffic databases. MaintenanceLast note on source connectorsI know it seems like I am only talking doom and gloom, but connectors are one of the best things to happen to real-time data processing. I’m trying to highlight that they are unfortunately not that easily to productionalize, but once you got them down, they work really, really well.Connector: High upfront work, medium maintenance (relatively), but doing it yourself would be worse.Sink Connectors"
    } ,
  
    {
      "title"       : "Starting a Journey",
      "category"    : "",
      "tags"        : "personal, intro",
      "url"         : "./intro.html",
      "date"        : "2023-09-18 04:37:00 +0000",
      "description" : "Why is Abraham doing this?",
      "content"     : "This is the beginning of my attempt to share stories from seeing multiple companies adopt an event-driven architecture, and the ensuing issues that come with it given the big mind-shift this entails. Over my time at Confluent, I’ve gotten to work with some really cool engineers and companies that affect our everyday lives and use Apache Kafka extensively. My hope with this blog is that by sharing a detailed account of very specific problems in the eventing world it will help future developers implement it more easily - or at least go a bit more educated into it.How often will you write?I will be kind to myself, and aim for a quarterly cadence. These are meant to be comprehensive, laid out blog posts of very specific situations. It will take time to get each right, and as we all know, time is the one precious thing we are always trying to get more of. I am in a beautiful stage of life where Work, Family, Friends, and whatnot are always asking for more of me. This blog is important, and as I learn more time management, I will aim to increase cadence.Where did you get the name of the blog?You may think from the name that I am an avid Tolkien reader, and while I do love the movies, I haven’t had the pleasure to read the books. I am a big fan of CGP Grey’s Youtube creations, and the name of this blog is a tribute to his now-deleted reddit username.Will you share company specifics?No. Privacy is important. No matter the blog, know I wouldn’t have written about it if I hadn’t seen it at multiple places, and the write up is a conglomeration of the experiences. No single blog will be connected to a single company.Thank YouLastly, I’d like to thank you for taking the time of reading so far, and if you are interested in the few semi-smart things I’ve got to say, I’d encourage you to sign up for email alerts on the blog posts. Thank you, and have a beautiful day!"
    } 
  
]
